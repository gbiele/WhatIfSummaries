---
title: "Chapter 11: Why model"
format:
  revealjs:
    embed-resources: true
---

## Bias-variance trade off

```{r}
bx = 1:4
x = sort(rep(bx,5))
y = poly(x,2,raw = TRUE) %*% c(2,1) + rnorm(length(x))*2
by = poly(bx,2,raw = TRUE) %*% c(2,1) + rnorm(length(bx))*2
m.l = lm(y~x)
p.l = predict(m.l, newdata= data.frame(x = bx), interval = "confidence")
m.f = lm(y~factor(x))
p.f = predict(m.f, newdata= data.frame(x = bx), interval = "confidence")
ylim = range(cbind(p.l,p.f))

par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
layout(matrix(c(1,1,1,2),nrow = 1))
plot(x,y, pch = 16, col = "grey", ylim = ylim)
points(-.05+bx,p.l[,1], pch = 16, col = "red", cex = 1.5)
points(.05+bx,p.f[,1], pch = 16, col = "blue", cex = 1.5)
points(bx,p.f[,1], pch = 16, cex = 1)

plot(x[x==3],y[x==3], pch = 16, col = "grey", ylab = "y", xlab = "x")
points(-.05+3,p.l[3,1], pch = 16, col = "red", cex = 1.5)
points(.05+3,p.f[3,1], pch = 16, col = "blue", cex = 1.5)
points(3,p.f[3,1], pch = 16, cex = 1)
arrows(x0 = 3-.2,x1 = 3-.2,y0 = p.f[3,1], y1 = p.l[3,1], length = 1/15, lwd = 3, code = 3, angle=90)
text(3-.2,mean(c(p.f[3,1],p.l[3,1])), "Bias", pos = 2, cex = 2)
```

## Bias-variance trade off

```{r}
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
layout(matrix(c(1,1,1,2),nrow = 1))

plot(x,y, pch = 16, col = "grey", ylim = ylim)
points(-.05+bx,p.l[,1], pch = 16, col = "red", cex = 1.5)
points(.05+bx,p.f[,1], pch = 16, col = "blue", cex = 1.5)
arrows(x0 = -.05+bx, y0 = p.l[,2],y1 = p.l[,3], length = 0, col = "red", lwd = 3)
arrows(x0 = .05+bx, y0 = p.f[,2],y1 = p.f[,3], length = 0, col = "blue", lwd = 3)
points(bx,p.f[,1], pch = 16, cex = 1)

plot(x[x==3],y[x==3], pch = 16, col = "grey", ylab = "", xlab = "x", ylim = range(c(p.l[3,],p.f[3,])))
points(-.1+3,p.l[3,1], pch = 16, col = "red", cex = 1.5)
points(.1+3,p.f[3,1], pch = 16, col = "blue", cex = 1.5)
arrows(x0 = -.1+3, y0 = p.l[3,2],y1 = p.l[3,3], length = 0, col = "red", lwd = 3)
arrows(x0 = .1+3, y0 = p.f[3,2],y1 = p.f[3,3], length = 0, col = "blue", lwd = 3)
points(3,p.f[3,1], pch = 16, cex = 1)
arrows(x0 = -.3+3, y0 = p.l[3,2],y1 = p.l[3,3], length = 1/10, lwd = 3, code = 3, angle=90)
arrows(x0 = .3+3, y0 = p.f[3,2],y1 = p.f[3,3], length = 1/10, lwd = 3, code = 3, angle=90)
text(3,max(c(p.l[3,],p.f[3,])), "Variance", pos = 3, cex = 2, xpd = TRUE, bg = "white")
```

## Identifying the "right" model

- the use of machine learning in causal inference is built on the idea the flexible ML models can reduce bias, while avoiding high variance
- personally, I doubt that using ML models out of the box (without expertly tuning hyper parameters) is the best path to finding the "best" model
- instead, combining domain knowledge with fitting flexible regularalized models and state of the are model comparison or model averaging are provide better guarantees to identfy models that neither over- nor underfit the data.
- 
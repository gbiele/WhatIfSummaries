---
title: "Chapter 11: Why model"
format:
  revealjs:
    embed-resources: true
    theme: [gb.css]
---

## Data cannot speak for themselves

-   causal effects are calculated as differences between conditional means for treatment levels
-   if the number of treatment levels is limited (dichotomous or categorical treatments) conditional means can be calculated without model directly from the data
-   if the number treatment levels is high direct calculations are impossible
-   then we can use statistical models, which incorporate assumptions about the **type of relationship** between treatment level and outcome
-   we also need models when the treatment is binary, but stratification or adjustment variables are continuous or higher dimensional [^1]

[^1]: Treatment and control group will, e.g., not have pairs with matching sex, height, and weight

## Parametric estimators of the conditional mean

1.  Assumption about the functional form (e.g. linear: $y = \alpha + \beta x$)
2.  Parameters ($\alpha$,$\beta$) govern the "expression" of the functional form

```{r}
#| fig-align: center
set.seed(125)
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
N = 25
x = rnorm(N)
y = 1.1 + x + rnorm(N)
plot(x,y,bty = "l", cex.lab = 1.5, cex.axis = 1.5)
m = lm(y~x)
abline(m)
arrows(x0 = x, x1 = x, y0 = predict(m), col = "grey",
       y = predict(m)+residuals(m), length = 0)
points(x,y, pch = 16)
txt = paste0("y = ", round(coef(m)[1],1), " + ", round(coef(m)[2],1),"y")
text(min(x),3,txt,pos = 4, cex = 2)
```

## Nonparametric estim. of the conditional mean

-   Nonparametric models do not assume a functional form
    -   each conditional mean has its own "parameter"

```{r}
#| fig-align: center
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
n_levels = 7
set.seed(122)
x = rep((1:n_levels),5)
y = sqrt(x) + rnorm(length(x))

df = data.frame(y = y, x = x, x.f = factor(x)) 
m.o = lm(y~poly(x,2), data = df)
p.o = predict(m.o, newdata = data.frame(x = 1:n_levels), level = .05, interval = "prediction")
m.f = lm(y~x.f, data = df)
p.f = predict(m.f, newdata = data.frame(x.f = factor(1:n_levels)), level = .05, interval = "prediction")
plot(x,y, pch = 16,cex.lab = 1.5, cex.axis = 1.5)
points(-.05+1:n_levels,p.o[,1], col = "red",pch = 16, cex = 2)
points(.05+1:n_levels,p.f[,1], col = "blue",pch = 16, cex = 2)
#arrows(x0 = -.05+1:n_levels, x1 = -.05+1:n_levels, y0 = p.o[,2], y1 = p.o[,3], col = "red", length = 0)
#arrows(x0 = .05+1:n_levels, x1 = .05+1:n_levels, y0 = p.f[,2], y1 = p.f[,3], col = "blue", length = 0)
```

## Smoothing

-   *linear and non-parametric models are extremes on a continuum*
    -   on one end the expected outcome for a treatment level depends on the outcomes observed at all treatment levels
    -   on the other end the expected outcome depends only on the outcomes observed at the focal treatment levels

```{r}
#| fig-align: center
library(splines)
N = 25
x = sort(rnorm(N))
y = x + rnorm(N)
par(mfrow = c(3,3), mar=c(0,0,0,0))
for (n in seq(1,25, by = 3)) {
  plot(x,y, xaxt = "n", yaxt = "n", xlab = "", ylab = "")
  #if (n>2) {m = gam(y~s(x,k = n)); lines(x,predict(m))}
  m2 = lm(y~bs(x,degree = n))
  lines(x,predict(m2), col = "blue")
  text(min(x),max(y),n, cex = 2)
}
```

## Non-linear relationships from "linear models"

We can model non-linear relationships by using modified predictors for the linear model"

```{r}
#| fig-align: center
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
x = seq(-3,3,.1)
par(mfrow = c(2,3),cex.lab = 1.5, cex.axis = 1.5, cex.main = 2)
plot(x,x,type = "l", ylab = "y", ylim = c(-6,8.5)); title(expression(y~"="~x))
plot(x,x*.5,type = "l", ylab = "y", ylim = c(-6,8.5)); title(expression(y~"="~0.5~x))
plot(x,x*.5 - x^2*.5,type = "l", ylab = "y", ylim = c(-6,8.5)) ; title(expression(y~"="~0.5~x~-~0.5~x^2))
plot(x,x^2,type = "l", ylab = "y", ylim = c(-6,8.5));title(expression(y~"="~x^2))
plot(x,x^2*.5,type = "l", ylab = "y", ylim = c(-6,8.5)); title(expression(y~"="~-0.5~x^2))
plot(x,-x + x^2*.2,type = "l", ylab = "y", ylim = c(-6,8.5)) ; title(expression(y~"="~-0.5~x~"+"~0.5~x^2))
```

## Modern statistics/ML estimates smoothness

Penalized splines use a penalty term to discount the fit of flexible models

```{r}
#| fig-align: center
set.seed(1234)
par(mfrow = c(2,2),mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
x = seq(1,15, length.out = 250)
y = sin(x) + rnorm(length(x))
set.seed(123456)
for (n in c(10,20,100,250)) {
  r = sort(sample(250,n))
  xx = x[r]
  yy = y[r]
  plot(xx,yy, pch = 16, col = "gray", ylab = "y", xlab = "x", xlim = c(1,15),
       main = paste("n =",n))
  nd = data.frame(xx = seq(1,15,length.out = 100))
  m.g = mgcv::gam(yy~s(xx))
  lines(nd$xx,predict(m.g, newdata = nd), col = "blue", lwd = 2)
  if (n == 10) {
    legend("topleft", bty = "n",
           legend = c("penlized splines"),
           lty = 1, lwd = 2,
           col = c("blue"))
  }
}

```

## Old statistics does not

```{r}
#| fig-align: center
par(mfrow = c(2,2),mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
set.seed(123456)
for (n in c(10,20,100,250)) {
  r = sort(sample(250,n))
  xx = x[r]
  yy = y[r]
  plot(xx,yy, pch = 16, col = "gray", ylab = "y", xlab = "x", xlim = c(-5,20),
       main = paste("n =",n))
  nd = data.frame(xx = seq(-5,20,length.out = 100))
  m.g = mgcv::gam(yy~s(xx))
  m.p = lm(yy~poly(xx,5))
  lines(nd$xx,predict(m.g, newdata = nd), col = "blue", lwd = 2)
  lines(nd$xx,predict(m.p, newdata = nd), col = "red", lwd = 2)
  if (n == 10) {
    legend("topleft", bty = "n",
           legend = c("penalized splines","polynomials"),
           lty = 1, lwd = 2,
           col = c("blue","red"))
  }
}

```

## Bias-variance trade-off

$$
\begin{align}
MSE &= \mathbb{E}[(y-\hat{y})^2] \\
MSE &= bias + variance + \sigma^2 \\
bias &= \mathbb{E}[y]-\mathbb{E}[\hat{y}] \\
variance &= \mathbb{E}[(\mathbb{E}[\hat{y}]-\hat{y})^2]
\end{align}
$$

-   $MSE$ = **total error**: <br> mean squared error between observed and predicted values
-   $bias$ = difference between the true and predicted mean
-   $variance$ = variability of predicted means over data samples

## $Bias$-variance trade-off

Difference between the true and predicted mean

```{r}
bx = 1:4
x = sort(rep(bx,15))
y = poly(x,2,raw = TRUE) %*% c(2,1) + rnorm(length(x))*2
by = poly(bx,2,raw = TRUE) %*% c(2,1)
m.l = lm(y~x)
p.l = predict(m.l, newdata= data.frame(x = bx), interval = "confidence")
m.f = lm(y~factor(x))
p.f = predict(m.f, newdata= data.frame(x = bx), interval = "confidence")
ylim = range(cbind(p.l,p.f))

bp = function() {
  par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01, cex.lab = 1.5, cex.axis = 1.5)
  layout(matrix(c(1,1,1,2),nrow = 1))
  plot(x,y, pch = 16, col = "grey", ylim = ylim, xaxt = "n", cex = 1.5, bty = "l")
  axis(1,at = 1:4)
  points(-.05+bx,p.l[,1], pch = 16, col = "red", cex = 2)
  points(.05+bx,p.f[,1], pch = 16, col = "blue", cex = 2)
  points(bx,by, pch = 16, cex = 2)
  legend("topleft", legend = c("y = 2x + x^2","y ~ factor(x)", "y ~ x"), col = c("black","blue","red"), pch = 16, cex = 2, bty = "n")
}
bp()
plot(x[x==3],y[x==3], pch = 16, col = "grey", ylab = "y", xlab = "x", xaxt = "n", cex = 1.5, bty = "l")
axis(1,at = 3)
points(-.05+3,p.l[3,1], pch = 16, col = "red", cex = 2)
points(.05+3,p.f[3,1], pch = 16, col = "blue", cex = 2)
points(3,p.f[3,1], pch = 16, cex = 2)
arrows(x0 = 3-.2,x1 = 3-.2,y0 = p.f[3,1], y1 = p.l[3,1], length = 1/15, lwd = 3, code = 3, angle=90)
text(3-.2,mean(c(p.f[3,1],p.l[3,1])), "Bias", pos = 2, cex = 2)
```

## Bias-$variance$ trade-off

Variability (standard error) of predicted means over samples.

```{r}
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01, cex.lab = 1.5, cex.axis = 1.5)
layout(matrix(c(1,1,1,2),nrow = 1))

bp()

plot(x[x==3],y[x==3], pch = 16, col = "grey", ylab = "", xlab = "x", 
     ylim = range(c(p.l[3,],p.f[3,])), xaxt = "n", cex = 1.5, bty = "l")
axis(1,at = 3)
points(-.1+3,p.l[3,1], pch = 16, col = "red", cex = 2)
points(.1+3,p.f[3,1], pch = 16, col = "blue", cex = 2)
arrows(x0 = -.1+3, y0 = p.l[3,2],y1 = p.l[3,3], length = 0, col = "red", lwd = 3)
arrows(x0 = .1+3, y0 = p.f[3,2],y1 = p.f[3,3], length = 0, col = "blue", lwd = 3)
points(3,p.f[3,1], pch = 16, cex = 1)
arrows(x0 = -.3+3, y0 = p.l[3,2],y1 = p.l[3,3], length = 1/10, lwd = 3, code = 3, angle=90)
arrows(x0 = .3+3, y0 = p.f[3,2],y1 = p.f[3,3], length = 1/10, lwd = 3, code = 3, angle=90)
text(3,max(c(p.l[3,],p.f[3,])), "Variance", pos = 3, cex = 2, xpd = TRUE, bg = "white")
```

## Bias-variance trade-off

```{r}
#| fig-height: 4
#| fig-width: 4
#| fig-align: center
var.l = predict(m.l, newdata= data.frame(x = bx), se.fit = TRUE)$se.fit
var.f = predict(m.f, newdata= data.frame(x = bx), se.fit = TRUE)$se.fit
bias.l = (by-predict(m.l, newdata= data.frame(x = bx)))^2
bias.f = (by-predict(m.f, newdata= data.frame(x = bx)))^2

par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01, cex.lab = 1.25, cex.axis = 1.25)
plot(c(var.f,var.l),c(bias.f,bias.l), cex = .1,
     col = sort(rep(c("blue","red"),4)), xlim = c(.25,.45),
     pch = 16, ylab = "Bias",xlab = "Variance")
text(c(var.f,var.l),c(bias.f,bias.l), cex = 1, labels = paste0("x=",bx),
     col = sort(rep(c("blue","red"),4)))
legend("topleft", legend = c("y ~ factor(x)", "y ~ x"), 
       col = c("blue","red"), pch = 16, cex = 1, bty = "n")
```

## Identifying the "right" model

-   the use of machine learning in causal inference is built on the idea the flexible ML models can reduce bias, while avoiding high variance
-   personally, I doubt that using ML models out of the box (without expertedly tuning hyper parameters) is the best path to finding the "best" model
-   instead, combining domain knowledge with fitting flexible regularized models and state of the are model comparison or model averaging are provide better guarantees to identify models that neither over- nor underfit the data.

## The generalised linear model

-   Different types of data require different likelihood functions
    -   distributions that describe data from different (partially) random processes
-   The means of some likelihood functions are restricted, which required link functions

## Logistic regression for binary outcomes
::: columns
::: {.column width="50%"}
linear model:

$lp_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + ...$

**restriction** 

$0 < p < 1$ 

**logistic** link function:

$p_i = exp(lp_i) / (1 + exp(lp_i))$

**Binomial** likelihood
$L_i = binomial(y_i, p_i)$
:::

::: {.column width="50%"}
```{r}
#| fig-align: center
#| fig-width: 5
#| fig-height: 5
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,
    cex.lab = 1.5, cex.axis = 1.5)
x = rnorm(150,sd = 5)
linear_predictor = 00 + 0.5*x
probability = boot::inv.logit(linear_predictor)
plot(linear_predictor, probability, ylim = c(0,1), pch = 16)
abline(h = c(0,1), col = "red")
```
:::
:::


## Poisson regression for counts

::: columns
::: {.column width="50%"}
linear model:

$lp_i = \alpha + \beta_1 x_i + \beta_2 x_i^2 + ...$

**restriction**

$\mu \geq 0$ 

**log** link function:

$\mu_i = exp(lp_i)$

**Poisson** likelihood
$L_i = poisson(y_i, \mu_i)$
:::

::: {.column width="50%"}
```{r}
#| fig-align: center
#| fig-width: 5
#| fig-height: 5
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,
    cex.lab = 1.5, cex.axis = 1.5)
x = rnorm(150,sd = 3)
linear_predictor = 00 + 0.5*x
count = exp(linear_predictor)
plot(linear_predictor, count, pch = 16)
abline(h = c(0), col = "red")
```
:::
:::




---
title: "Chapter 11: Why model"
format:
  revealjs:
    embed-resources: true
    theme: [gb.css]
---

## Data cannot speak for themselves 

- causal effects are calculated as differences between conditional means for treatment levels
- if the number of treatment levels is limited (dichotomous or categorical treatments) conditional means can be calculated without model directly from the data
- if the number treatment levels is high direct calculations are impossible
- in then we can use statistical models, which incorporate assumptions about the **type of relationship** between treatment level and outcome
- we also need models when the treatment is binary, but stratification or adjustment variables are continuous or higher dimensional ^[Treatment and control group will e.g. not have pairs with matching sex, height, and weight]

## Parametric estimators of the conditional mean

1. Assumption about the functional form (e.g. linear)
2. Parameters govern the "expression" of the functional form
  
```{r}
#| fig-align: center
set.seed(125)
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
N = 25
x = rnorm(N)
y = 1.1 + x + rnorm(N)
plot(x,y,bty = "l", cex.lab = 1.5, cex.axis = 1.5)
m = lm(y~x)
abline(m)
arrows(x0 = x, x1 = x, y0 = predict(m), col = "grey",
       y = predict(m)+residuals(m), length = 0)
points(x,y, pch = 16)
txt = paste0("y = ", round(coef(m)[1],1), " + ", round(coef(m)[2],1),"y")
text(min(x),3,txt,pos = 4, cex = 2)
```

## Nonparametric estim. of the conditional mean

- Nonparametric models do not assume a functional form 
  - each conditional mean has its own "parameter"

```{r}
#| fig-align: center
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
n_levels = 7
set.seed(122)
x = rep((1:n_levels),5)
y = sqrt(x) + rnorm(length(x))

df = data.frame(y = y, x = x, x.f = factor(x)) 
m.o = lm(y~poly(x,2), data = df)
p.o = predict(m.o, newdata = data.frame(x = 1:n_levels), level = .05, interval = "prediction")
m.f = lm(y~x.f, data = df)
p.f = predict(m.f, newdata = data.frame(x.f = factor(1:n_levels)), level = .05, interval = "prediction")
plot(x,y, pch = 16,cex.lab = 1.5, cex.axis = 1.5)
points(-.05+1:n_levels,p.o[,1], col = "red",pch = 16, cex = 2)
points(.05+1:n_levels,p.f[,1], col = "blue",pch = 16, cex = 2)
#arrows(x0 = -.05+1:n_levels, x1 = -.05+1:n_levels, y0 = p.o[,2], y1 = p.o[,3], col = "red", length = 0)
#arrows(x0 = .05+1:n_levels, x1 = .05+1:n_levels, y0 = p.f[,2], y1 = p.f[,3], col = "blue", length = 0)
```

## Smoothing

- linear and non-parametric models are extremes on a continuum
  - on one end the expected outcome for a treatment level depends on the outcomes observed at all treatment levels
  - on the other end the expected outcome for a treatment level depends only on the outcomes observed at the focal treatment levels

```{r}
#| fig-align: center
library(splines)
N = 25
x = sort(rnorm(N))
y = x + rnorm(N)
par(mfrow = c(3,3), mar=c(0,0,0,0))
for (n in seq(1,25, by = 3)) {
  plot(x,y, xaxt = "n", yaxt = "n", xlab = "", ylab = "")
  #if (n>2) {m = gam(y~s(x,k = n)); lines(x,predict(m))}
  m2 = lm(y~bs(x,degree = n))
  lines(x,predict(m2), col = "blue")
  text(min(x),max(y),n, cex = 1.5)
}
```

## Non-linear relationships from "linear models"

```{r}
#| fig-align: center
par (mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
x = seq(-3,3,.1)
par(mfrow = c(2,3),cex.lab = 1.5, cex.axis = 1.5, cex.main = 2)
plot(x,x,type = "l", ylab = "y", ylim = c(-6,8.5)); title(expression(y~"="~x))
plot(x,x*.5,type = "l", ylab = "y", ylim = c(-6,8.5)); title(expression(y~"="~0.5~x))
plot(x,x*.5 - x^2*.5,type = "l", ylab = "y", ylim = c(-6,8.5)) ; title(expression(y~"="~0.5~x~-~0.5~x^2))
plot(x,x^2,type = "l", ylab = "y", ylim = c(-6,8.5));title(expression(y~"="~x^2))
plot(x,x^2*.5,type = "l", ylab = "y", ylim = c(-6,8.5)); title(expression(y~"="~-0.5~x^2))
plot(x,-x + x^2*.2,type = "l", ylab = "y", ylim = c(-6,8.5)) ; title(expression(y~"="~-0.5~x~"+"~0.5~x^2))
```

## Modern statistics/ML estimates smoothness

```{r}
#| fig-align: center
set.seed(1234)
par(mfrow = c(2,2),mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
x = seq(1,15, length.out = 250)
y = sin(x) + rnorm(length(x))
set.seed(123456)
for (n in c(10,20,100,250)) {
  r = sort(sample(250,n))
  xx = x[r]
  yy = y[r]
  plot(xx,yy, pch = 16, col = "gray", ylab = "y", xlab = "x", xlim = c(1,15),
       main = paste("n =",n))
  nd = data.frame(xx = seq(1,15,length.out = 100))
  m.g = mgcv::gam(yy~s(xx))
  lines(nd$xx,predict(m.g, newdata = nd), col = "blue", lwd = 2)
  if (n == 10) {
    legend("topleft", bty = "n",
           legend = c("splines"),
           lty = 1, lwd = 2,
           col = c("blue"))
  }
}

```

## Old statistics does not

```{r}
#| fig-align: center
par(mfrow = c(2,2),mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01)
set.seed(123456)
for (n in c(10,20,100,250)) {
  r = sort(sample(250,n))
  xx = x[r]
  yy = y[r]
  plot(xx,yy, pch = 16, col = "gray", ylab = "y", xlab = "x", xlim = c(-5,20),
       main = paste("n =",n))
  nd = data.frame(xx = seq(-5,20,length.out = 100))
  m.g = mgcv::gam(yy~s(xx))
  m.p = lm(yy~poly(xx,5))
  lines(nd$xx,predict(m.g, newdata = nd), col = "blue", lwd = 2)
  lines(nd$xx,predict(m.p, newdata = nd), col = "red", lwd = 2)
  if (n == 10) {
    legend("topleft", bty = "n",
           legend = c("splines","polynomials"),
           lty = 1, lwd = 2,
           col = c("blue","red"))
  }
}

```

## The bias-variance trade-off

## The generalised linear model

- Different types of data require different likelihood functions
  - distributions that describe data from different processes
- The means of some likelihood functions are restricted, which required link functions

## Logistic regression for binary outcomes

**Binomial** likelihood function + **logistic** link function

```{r}
#| fig-align: center
#| fig-width: 5
#| fig-height: 5
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,
    cex.lab = 1.5, cex.axis = 1.5)
x = rnorm(150,sd = 5)
linear_predictor = 00 + 0.5*x
probability = boot::inv.logit(linear_predictor)
plot(linear_predictor, probability, ylim = c(0,1), pch = 16)
abline(h = c(0,1), col = "red")
```

## Poisson regression for count

**Poisson** likelihood function + **log** link function

```{r}
#| fig-align: center
#| fig-width: 5
#| fig-height: 5
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,
    cex.lab = 1.5, cex.axis = 1.5)
x = rnorm(150,sd = 3)
linear_predictor = 00 + 0.5*x
count = exp(linear_predictor)
plot(linear_predictor, count, pch = 16)
abline(h = c(0), col = "red")
```
